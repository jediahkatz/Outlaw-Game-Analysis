\documentclass{article}

\usepackage{enumerate}
\usepackage{kbordermatrix}
\usepackage{graphicx}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{algorithm2e}
\usepackage{tikz}
\usetikzlibrary{automata, positioning, arrows, calc}

\title{Frequentist and Bayesian Analyses of the Board Game \textit{Outlaw}}
\author{Jediah Katz}
\date{February 21, 2018}

\renewcommand{\qed}{$\hfill \blacksquare$}
\newcommand{\T}{\textbf{\textit{T}}}
\newcommand{\x}{\vec{x}}
\renewcommand{\arraystretch}{1.3}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}

\begin{document}
	\maketitle
	\pagenumbering{arabic}
	\begin{abstract}
		A variant of the dice-based board game \textit{Outlaw}, designed by Phil Horswell, was analyzed 
	\end{abstract}

	\section{Background}
	Two of the most prominent understandings of probability are the frequentist and Bayesian interpretations. Frequentist probability, most generally, defines probability as the relative frequency of an event's occurrence in a suitable number of repetitions of a process. In contrast, Bayesian probability, most generally, defines probability as the degree of confidence in an event's occurrence held by a suitable agent. Each of these interpretations of probability corresponds to its own means of modeling a process in order to determine the probability that particular events will occur.
	
	\subsection{Monte-Carlo Simulations}
	Imagine a rather boring game in which a single player rolls one die each turn and adds its value to the the score, which begins at zero. The player wins if the score is greater than twenty after five turns, and loses otherwise. Suppose we are interested in determining the probability that the player wins the game. In the frequentist interpretation of probability, an intuitive solution is to simply play the game many times, recording whether the player wins or loses.
	
	However, it is often impractical and time-consuming to manually repeat this process a suitable number of times in order for the relative frequency to converge. Fortunately, the use of a computer can greatly speed up the repetition of trials. By creating a simulation to model the playing of a game, millions of trials can be run in very little time, a process known as the Monte-Carlo method. 
	
	It is worth noting that assumptions must often be made in the process of building a computer simulation. For example, in order to simulate the rolling of a die, we assume that a uniform pseudo-random number generator that generates integers in the interval {[1,6]} correctly models the behavior of a die.
	
	\subsection{Absorbing Markov Chains}
	For some more complex processes, however, even a computer simulation may not be feasible. Events with very low probability, such as ten dice each rolling a six, can easily require hundreds of millions of trials to occur in a simulation. Fortunately, Bayesian methods of probability modeling provide an elegant solution to this problem. As Bayesian probability is not constrained by the idea of frequency, we can use our assumptions to mathematically model processes.


	One of the most valuable tools for modeling in the Bayesian's belt is the Markov chain. 

\begin{defn}
For some finite state space $I$, we define a discrete-time Markov chain with initial distribution $\lambda$ and transition matrix $\T$ as a  sequence of random variables $(X_n)_{n \geq 0}$ such that, for $n \geq 0$ and $i_0, \dots, i_{n+1} \in I$,
	\begin{enumerate}[(i)]
	\item $\lambda_{i_0} = \Pr[X_0 = i_0]$  
	\item $\T_{i_n, i_{n+1}} = \Pr[X_{n+1} = i_{n+1} ~|~ {X_0 = i_0}, \dots, X_{n}=i_n]$

	\hspace{3.7em}$ = \Pr[X_{n+1} = i_{n+1} ~|~ X_{n} = i_n]$
	\end{enumerate}
\end{defn}
We can represent $(X_n)_{n \geq 0}$ with the convenient notation $Markov(\lambda, \T)$. Equivalent to (ii), the probability of transitioning to state $j$ from state $i$ is given by $\T_{i, j}$ and is independent of all previous states; this is known as memorylessness or the Markov property. Also, note that $\T$ is a right stochastic matrix, meaning that for any $i \in I$, \[\sum_{j\in I} \T_{i, j} = 1;\] that is, the probabilities in each row of $\T$ sum to 1. Finally, note that we represent the distribution $\lambda$ as a stochastic vector.
\\
	
	An absorbing Markov chain is simply a Markov chain for which there exists a state $i \in I$ such that $\T_{i,i} = 1$. Equivalently, an absorbing Markov chain contains at least one state from which transition to another state is impossible. Absorbing Markov chains are especially useful for modeling games, as winning a game is typically an absorbing state.
	
	Consider the game represented by the directed graph below. The player begins at state $a$. Each turn, the player rolls a die and then transitions across the corresponding edge, if one exists. The player wins upon reaching state $c$.
	
	\[
	\begin{tikzpicture}[shorten >=1pt, node distance=3cm,auto, initial text={}]
      \node[state,initial] (a) {$a$};
      \node[state] (b) [right of=a] {$b$};
      \node[state,accepting] (c) [right of=b] {$c$};
      \path[->]
                (a) edge[bend left] node {1, 2, 3, 4} (b)
                (b) edge[bend left] node {6} (c)
                (b) edge[bend left] node {1, 2} (a);
     \end{tikzpicture}
	\]
	
	Since the probability of transitioning between states only depends on the current state, this game is an ideal candidate for a Markov chain with state space $I = \{a, b, c\}$. Since the player begins at state $a$ with probability 1, then we can represent this game as a Markov-chain $Markov(\lambda, \T)$ such that
	\[\lambda = \kbordermatrix{ ~ &a &b &c \cr
		& 1 & 0 & 0},\]
	\[\T = \kbordermatrix{~ &a & b & c \cr
		a&\frac{2}{6} &  \frac{4}{6}  & 0 \cr
		b& \frac{2}{6}  &  \frac{3}{6} & \frac{1}{6} \cr
		c& 0 & 0 & 1}\]
	At this point, it is very simple to determine the probability that the player is at any state in $i \in I$ after $n$ turns, for some $n \in \mathbb{N}$.
\\
	\begin{thm}
	If $(X_n)_{n \geq 0}$ is $Markov(\lambda, \T)$, then for all $n \geq 0$, $i_0, \dots, i_n \in I$, \[\Pr[X_0 = i_0, X_1 = i_1, \dots, X_n = i_n] = \lambda_{i_0}\T_{i_0, i_1}\T_{i_1,i_2}, \dots, \T_{i_{n-1}, i_n}\]
	\end{thm}
	
	\begin{proof} Since $(X_n)_{n \geq 0}$ is $Markov(\lambda, \T)$, then
	\begin{align*}
	\Pr[&X_0 = i_0, X_1 = i_1, \dots, X_n = i_n]\\
	 &= \Pr[X_0 = i_0]\Pr[X_1 = i_1 | X_0 = i_0]\dots\Pr[X_n = i_n | X_0 = i_0, \dots X_{n-1} = i_{n-1}]\\
	 &= \lambda_{i_0}\T_{i_0, i_1}\T_{i_1,i_2}, \dots, \T_{i_{n-1}, i_n}
	\end{align*}
	\end{proof}
	It can be shown that the converse of this statement holds as well.
	
	\begin{thm} If $(X_n)_{n \geq 0}$ is $Markov(\lambda, \T)$, then for any $n \geq 0$, $j \in I$,
	\[\Pr[X_n = j] = (\lambda \T^{\,n})_j\]
	\end{thm}
	
	\begin{proof}
	The probability $\Pr[X_n = j]$ that we are on state $j$ after $n$ steps is equal to the sum of the probabilities of each permutation of states from steps $0$ through $n-1$. Therefore,
	\begin{align*}
	\Pr[X_n = j] &= \sum_{i_0 \in I} \dots \sum_{i_{n-1} \in I} \Pr[X_0 = i_0, \dots, X_{n-1} = i_{n-1}, X_n = j]\\
	&= \sum_{i_0 \in I} \dots \sum_{i_{n-1} \in I} \lambda_{i_0}\T_{i_0, i_1}\dots\T_{i_{n-1}, j} \tag{by Theorem 1}\\
	&= (\lambda \T^n)_j
	\end{align*}
	\end{proof}
	
		Returning to our game, we can use this identity to determine the probability of being at each state after ten turns.
		\begin{align*}
		\Big[\Pr[X_{10} = i] ~|~ i \in I\Big] &= 
		\begin{bmatrix}
		\Pr[X_{10} = a] & \Pr[X_{10} = b] & \Pr[X_{10} = c]
		\end{bmatrix}\\ &=
		\begin{bmatrix}
		1 & 0 & 0
		\end{bmatrix}
		\begin{bmatrix}
			\frac{2}{6} & \frac{4}{6} & 0           \\
			\frac{2}{6} & \frac{3}{6} & \frac{1}{6} \\
			0           & 0			  & 1
		\end{bmatrix}
		^{10}\\
		&=	
		\begin{bmatrix}
		\frac{1033729}{7558272} & \frac{3486025}{15116544} & \frac{3187687}{5038848}
		\end{bmatrix}
		\approx \begin{bmatrix}
		.14 & .23 & .63
		\end{bmatrix}
		\end{align*}
	\\
	Thus $\Pr[X_{10} = a] \approx .14$, $\Pr[X_{10} = b] \approx .23$, and $\Pr[X_{10} = c] \approx .63$.
	
	\subsection{The \textit{Outlaw} Game}
	We analyze a variant of \textit{Outlaw}, a dice-based board game intended for 2 to 3 players, designed and published online by Phil Horswell. The \textit{Outlaw} game board features the set of 12 outlaws $O = \{o_4, \dots, o_{9}, o_{19}, \dots, o_{24}\}$, each with a unique identifier in the range $[4, 9] \cup [19, 24]$. Players take turns rolling 4 dice; if the sum of the dice is equal to $k$ for $k \in [4, 9] \cup [19, 24]$, then the player captures outlaw $o_k$. Each outlaw comes with an associated cash bounty.
	
	\begin{center}
	\includegraphics[scale=0.5]{outlaw_board}
	\captionof{figure}{The \textit{Outlaw} game board.}
	\end{center}
	
	In our ``Jailbreak'' variant of the game, if a player $P$ rolls an outlaw who has already been captured by a distinct player $P'$, then $P$ will steal that outlaw from away from $P'$. Only one player can have possession of any particular outlaw at one time. If a player $P$ has possession of $n$ distinct outlaws at once, then $P$ wins the game.
	
	\begin{defn}
	We let $G = Jailbreak(p, n)$ be a game of ``Jailbreak'' \textit{Outlaw} such that $G$ has $p$ players, and a player $P$ wins if $P$ has possession of $w$ distinct outlaws at once, for $p \geq 1$ and $1 \leq w \leq 12$.
	\end{defn}
	
	\section{Analysis of $Jailbreak(p, w)$}

	We begin by analyzing $G = Jailbreak(p, w)$. Our analysis centers upon the length of the game; that is, how many turns are required in expectation until any player wins?

	\subsection{Frequentist Analysis}
	We present the following psuedocode to simulate a run of $G$. The actual code is available in the resources folder; C++ was used for its speed. Note that we define a maximum number of turns to ensure that the procedure terminates.
	\begin{algorithm}
	\SetKwInOut{Input}{Input}
	\Input{$p$ : number of players\\ $w$ : number of outlaws to win\\ $m$ : maximum number of turns}

	\SetKwInOut{Output}{Output}
	\Output{number of turns in simulated run of $Jailbreak(p, w)$}
	\ForEach{outlaw $o$}{
		owner[$o$] $\gets$ None
	}
	\ForEach{player $P$}{
		numOutlaws[$P$] $\gets$ 0
	}
	turn $\gets$ 0\\
	\While{turn $< m$}{
		turn $\gets$ turn + 1\\
		\ForEach{player $P$}{
			$k \gets$ randInt(1, 6) + randInt(1, 6) + randInt(1, 6) + randInt(1, 6)\\
			\If{$4 \leq k \leq 9$ or $19 \leq k \leq 24$}{
				numOutlaws[$P$] $\gets$ numOutlaws[$P$] + 1\\
				prevOwner $\gets$ owner[$o_k$]\\
				\If{{\upshape oldOwner} not \upshape None}{
					numOutlaws[prevOwner] $\gets$ numOutlaws[prevOwner] - 1
				}
				owner[$o_k$] $\gets P$\\
				\If{{\upshape numOutlaws[$P$]} $\geq w$}{
					\Return{\upshape turn}
				}
			}
		}
	}
	\Return{``turn limit reached''}
	\end{algorithm}

	\subsection{Bayesian Analysis}

	\subsubsection{Counting Dice Combinations}
	To begin our analysis, we attempt to calculate the probability of capturing each outlaw or not capturing any outlaw in one turn. We assume that the probability of rolling any number on a die is $\frac{1}{6}$, and we also assume that dice rolls are independent.

	We define $E_{o_k}$ to be the event that a player $P$ captures outlaw $o_k$ after rolling the dice, and we define $E_\varnothing$ to be the event that $P$ misses; that is, $P$ does not capture any outlaw. 

	Let our sample space $\Omega$ be the set of all arrangements of four dice. Let $A_k$ be the set of arrangements of four dice such that their sum is $k$. Since $\Omega$ is a uniform sample space, we can see that for any $k \in [4, 9] \cup [19, 24]$, 
\begin{align}\Pr[E_{o_k}] = \Pr[\text{sum of 4 dice rolls is $k$}] = \frac{|A_k|}{|\Omega|}\end{align}

	We can see that each die has 6 possible states, so $|\Omega| = 6^4 = 1296$. To count $|A_k|$, we can consider the problem of distributing $k$ indistinguishable units to the 4 dice. Since each die must have a positive value, we must distribute 1 unit to each of the 4 dice, leaving us with $k-4$ units in total to distribute. For $4 \leq k \leq 9 \implies 0 \leq k - 4 \leq 5$, we cannot distribute more than 5 additional units to one die, which would leave it with an value greater than 6. Therefore, by the stars and bars technique, $|A_k| = \binom{k - 4 + 4 - 1}{4 - 1} = \binom{k - 1}{3}$ for $4 \leq k \leq 9$.

	Therefore, we have
	\begin{align*}
		&|A_4| = \tbinom{3}{3} = 1\\
		&|A_5| = \tbinom{4}{3} = 4\\
		&|A_6| = \tbinom{5}{3} = 10\\
		&|A_7| = \tbinom{6}{3} = 20\\
		&|A_8| = \tbinom{7}{3} = 35\\
		&|A_9| = \tbinom{8}{3} = 56
	\end{align*}

	Next, we notice that the sum of 4 dice is at most $6 \times 4 = 24$. Then we can also count $|A_k|$ by considering the problem of removing $24 - k$ industinguishable units in total from the 4 dice. For $19 \leq k \leq 24 \implies 0 \leq 24 - k \leq 5$, we cannot remove more than 5 units from one die, which would leave it with a value less than 1. Therefore, we can imagine that we are distributing $24 - k$ indistinguishable spots of white paint, each of which can remove one dot from a die, to the 4 dice. Then by the stars and bars technique again, $|A_k| = \binom{24 - k + 4 - 1}{4 - 1} = \binom{27 - k}{3}$ for $19 \leq k \leq 24$.

	Therefore, we have
	\begin{align*}
		&|A_{19}| = \tbinom{27 - 19}{3} = \tbinom{8}{3} = 56\\
		&|A_{20}| = \tbinom{27 - 20}{3} = \tbinom{7}{3} = 35\\
		&|A_{21}| = \tbinom{27 - 21}{3} = \tbinom{6}{3} = 20\\
		&|A_{22}| = \tbinom{27 - 22}{3} = \tbinom{5}{3} = 10\\
		&|A_{23}| = \tbinom{27 - 23}{3} = \tbinom{4}{3} = 4\\
		&|A_{24}| = \tbinom{27 - 24}{3} = \tbinom{3}{3} = 1
	\end{align*}

	Finally, by Equation (1) from above,
	\begin{align*}
	\Pr[E_{o_4}] &= \Pr[E_{o_{24}}] = \tfrac{1}{1296}\\
	\Pr[E_{o_5}] &= \Pr[E_{o_{23}}] = \tfrac{4}{1296}\\
	\Pr[E_{o_6}] &= \Pr[E_{o_{22}}] = \tfrac{10}{1296}\\
	\Pr[E_{o_7}] &= \Pr[E_{o_{21}}] = \tfrac{20}{1296}\\
	\Pr[E_{o_8}] &= \Pr[E_{o_{20}}] = \tfrac{35}{1296}\\
	\Pr[E_{o_9}] &= \Pr[E_{o_{19}}] = \tfrac{56}{1296}\\
	\Pr[E_\varnothing] &= 1 - \Pr[E_{o_4} \cup \dots \cup E_{o_9} \cup E_{o_{19}} \cup \dots \cup E_{o_{24}}]\\
		&= 1 - \frac{2(1 + 4 + 10 + 20 + 35 + 56)}{1296}\\
		&= 1 - \frac{252}{1296} = \frac{1044}{1296}
	\end{align*}

	% Todo: explain why we only keep track of player 1's state - is it an approximation or not?
	\subsubsection{Discrete-Time Markov Chain Construction}
We attempt to construct a Markov chain $(X_n)_{n \geq 0} = Markov(\lambda, \T)$ to describe $G$. We must first choose an appropriate state space $I$. Clearly, we cannot let a state simply represent the number of outlaws that $P_1$ controls, because different outlaws have different probabilities of capture, so it matters precisely which outlaws $P_1$ controls.  Therefore, we let $I = \{S \in \mathcal{P}(O) ~\big|~ |S| \leq w\}$; that is, each state represents the subset of distinct outlaws that $P_1$ controls. We don't need to consider subsets of more than $w$ outlaws, because once $P_1$ collects $w$ outlaws then the game is over.

	However, note that since the game ends when $P_1$ collects \textit{any} subset of $w$ outlaws, then it doesn't matter which $w$ outlaws $P_1$ controls. Therefore, for all states $S$ such that $|S| = w$, we can combine them into one winning state $F$. Thus we can instead let $I = \{S \in \mathcal{P}(O) ~\big|~ |S| < w\} \cup \{F\}$.

	% Future note - we can simplify all of the n-subsets into one state!
	% To consider - use Myhill-Nerode to show that # of states is minimized? That would be dope
	Now that we have chosen our state space, we must choose our initial distribution $\lambda$. At the start of the game, each player controls no outlaws; therefore, \[\lambda_\emptyset = \Pr[X_0 = \emptyset] = 1, \text{ and }\lambda_S = \Pr[X_0 = S] = 0,\] for all $S \in I$ s.t. $S \neq \emptyset$.

	Finally, we must define our transition matrix $\T$. But what constitutes a ``transition'' of our Markov chain? Even though our state space only contains the possible subsets of outlaws that $P_1$ controls, we cannot consider only $P_1$'s dice rolls as our transitions. In ``Jailbreak'' \textit{Outlaw}, the other players' dice rolls can also affect the $P_1$'s current subset of outlaws, because other players can steal away an outlaw from $P_1$. Therefore every player's dice roll must be a transition.

	However, note that our transition matrix is different depending on whether it is currently $P_1$'s turn or another player's turn. For now, we consider these cases separately. We will define two transition matrices, $\T$ and $\T'$, to describe the probabilities of each transition on $P_1$'s turn and on another player's turn, respectively.\\


	In the case that it is $P_1$'s turn, and the current state is $S \in I$, then we can determine the probability of transitioning to any other state:

	If $S$ is the winning state, that is, if $S = F$, then the game is over, so we transition back to $F$ with probability 1. Otherwise, if $S \neq F$:

	We can never lose control of an outlaw on our turn, so the probability of transitioning to any state containing fewer outlaws is 0. We also cannot capture more than one outlaw per turn, so the probability of transitioning to any state containing more than one new outlaw is 0.

	We transition back to $S$ if we roll any outlaw we already control, or if we don't capture any outlaw; therefore the probability of transitioning back to $S$ is the probability of capturing any outlaw $o \in S$ or rolling no outlaw.

	We can only transition to a new state by capturing a new outlaw. If we only need to capture one more outlaw to win, then the probability of transitioning to $F$ is the probability that we capture any new outlaw. Otherwise, for any outlaw $o \not \in S$, the probability of transitioning to any new state $S' = S \cup \{o\}$ is the probability of capturing $o$.

	Therefore, for any states $S, S' \in I$, we define \[\T_{S, S'} =
	\begin{cases}
	0 &\text{ if $|S'| < |S|$ or $|S'| > |S| + 1$}\\
	0 &\text{ if $S = F \neq S'$}\\
	1 &\text{ if $S = F = S'$}\\
	\sum\limits_{o \not \in S} \Pr[E_o] &\text{ if $|S| = w - 1$ and $S' = F$}\\
	\Pr[E_\varnothing] + \sum\limits_{o \in S} \Pr[E_o] &\text{ if $S' = S \neq F$}\\
	\Pr[E_o] &\text{ if $S' = S \cup \{o\}$}
	\end{cases}\]

	In the case that it is not $P_1$'s turn, and the current state is $S \in I$, then we can determine the probability of transitioning to any other state:

	If $S$ is the winning state, that is, if $S = F$, then the game is over, so we transition back to $F$ with probability 1. Otherwise, if $S \neq F$:

	We can never gain control of an outlaw on another player's turn, so the probability of transitioning to any state containing more outlaws is 0. Also, another player cannot steal away more than one outlaw per turn, so the probability of transitioning to any state containing less than one fewer outlaws is 0.
	
	
	
	\begin{thebibliography}{9}
		\bibitem{interpretations}
		H\'{a}jek, Alan, ``Interpretations of Probability'', The Stanford Encyclopedia of Philosophy (Winter 2012 Edition), Edward N. Zalta (ed.), \texttt{https://plato.stanford.edu/archives/win2012/entries/probability-interpret/}.
	\end{thebibliography}
\end{document}